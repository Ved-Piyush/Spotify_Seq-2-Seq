{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e1e042",
   "metadata": {},
   "source": [
    "## import the libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Activation, RepeatVector, \\\n",
    "                                    Permute, Multiply, Lambda, Concatenate, LSTM, TimeDistributed\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import backend as K\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172dd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu config\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24975be1",
   "metadata": {},
   "source": [
    "## Data Prep for train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb864496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ecb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the main path\n",
    "os.chdir(\"D:\\RecSys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files in one of the ten training chunks\n",
    "files = os.listdir(\"all_train/training_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33868e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbc45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the session logs\n",
    "# how many do you want to read\n",
    "empty = []\n",
    "for i in tqdm(range(0,1)):\n",
    "    sample_df_1 = pd.read_csv(\"all_train/training_set/\" + files[i])\n",
    "    empty.append(sample_df_1)\n",
    "    print(files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0db6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del the read sample df from the loop to save memory\n",
    "del(sample_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled data\n",
    "# concatenate into dataframe\n",
    "train = pd.concat(empty, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec90219",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85187683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique sessions\n",
    "train.session_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e33d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.track_id_clean.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4096ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function helps with the padding and the sequences to same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequence, pad_to_this_length, pre): \n",
    "    if len(sequence) < pad_to_this_length:\n",
    "        if pre == True:\n",
    "            add = np.repeat(0, pad_to_this_length - len(sequence)).astype(str).tolist()\n",
    "            combo = [add, sequence]\n",
    "            combo = [item1 for item in combo for item1 in item]\n",
    "        else:\n",
    "            add = np.repeat(0, pad_to_this_length - len(sequence)).astype(str).tolist()\n",
    "            combo = [sequence,add]\n",
    "            combo = [item1 for item in combo for item1 in item]\n",
    "            \n",
    "    else: \n",
    "        combo = sequence\n",
    "    return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749165df",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016928eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the session logs\n",
    "# how many do you want to read\n",
    "empty = []\n",
    "for i in tqdm(range(4,5)):\n",
    "    sample_df_1 = pd.read_csv(\"all_train/training_set/\" + files[i])\n",
    "    print(files[i])\n",
    "    empty.append(sample_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del the read sample df from the loop to save memory\n",
    "del(sample_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c70f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled data\n",
    "# concatenate into dataframe\n",
    "valid = pd.concat(empty, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique sessions\n",
    "valid.session_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c191e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.track_id_clean.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by first session id and then session position\n",
    "valid = valid.sort_values([\"session_id\", \"session_position\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the data\n",
    "valid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies for session length\n",
    "valid.session_length.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sequences(data): \n",
    "    # sort by first session id and then session position\n",
    "    data = data.sort_values([\"session_id\", \"session_position\"])\n",
    "\n",
    "    data[\"row_number\"] = data.groupby(['session_id']).cumcount()\n",
    "\n",
    "    data[\"session_row_number\"] = data[\"session_id\"] + data[\"row_number\"].astype(str)\n",
    "\n",
    "#     train.head()\n",
    "\n",
    "    train_session_lengths = data.groupby(\"session_id\")[[\"session_length\"]].first()\n",
    "\n",
    "    train_session_lengths_half = train_session_lengths.copy()\n",
    "\n",
    "    train_session_lengths_half[[\"session_length\"]] = train_session_lengths_half[[\"session_length\"]]//2\n",
    "\n",
    "#     train_session_lengths_half\n",
    "\n",
    "    repeats = np.repeat(train_session_lengths_half.index, train_session_lengths_half.session_length)\n",
    "\n",
    "    repeats_df = pd.DataFrame(repeats)\n",
    "\n",
    "    repeats_df[\"row_number\"] =  repeats_df.groupby(\"session_id\").cumcount()\n",
    "\n",
    "#     repeats_df.head()\n",
    "\n",
    "    repeats_df[\"session_row_number\"] = repeats_df[\"session_id\"] + repeats_df[\"row_number\"].astype(str)\n",
    "\n",
    "    train_first_half = pd.merge(data, repeats_df[\"session_row_number\"], how = \"inner\", left_on = [\"session_row_number\"], \n",
    "        right_on = [\"session_row_number\"])\n",
    "\n",
    "#     train_first_half\n",
    "\n",
    "    train_first_half_check = train_first_half.groupby(\"session_id\")[[\"session_length\"]].count()\n",
    "\n",
    "#     train_first_half_check.head()\n",
    "\n",
    "#     np.mean(train_session_lengths_half[\"session_length\"] == train_first_half_check[\"session_length\"])\n",
    "\n",
    "#     train.index\n",
    "\n",
    "#     train_first_half.index\n",
    "\n",
    "    train_second_half = data[~data.session_row_number.isin(train_first_half.session_row_number)]\n",
    "\n",
    "#     train_first_half.head()\n",
    "\n",
    "    train_first_half_seqs = pd.DataFrame(train_first_half.groupby(\"session_id\")[\"track_id_clean\"].apply(list)).reset_index()\n",
    "\n",
    "    train_second_half_seqs = pd.DataFrame(train_second_half.groupby(\"session_id\")[\"track_id_clean\"].apply(list)).reset_index()\n",
    "\n",
    "    train_all_seqs = pd.DataFrame(data.groupby(\"session_id\")[\"track_id_clean\"].apply(list)).reset_index()\n",
    "\n",
    "#     train_all_seqs.shape\n",
    "\n",
    "#     train_second_half_seqs.shape\n",
    "\n",
    "#     train_all_seqs.shape\n",
    "\n",
    "    return train_first_half_seqs, train_second_half_seqs, train_all_seqs, \\\n",
    "            np.mean(train_all_seqs.session_id.values == train_first_half_seqs.session_id.values),  \\\n",
    "           np.mean(train_all_seqs.session_id.values == train_second_half_seqs.session_id.values), \\\n",
    "           np.mean(train_first_half_seqs.session_id.values == train_second_half_seqs.session_id.values),\\\n",
    "            train_first_half, \\\n",
    "            train_second_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_first_half_seqs, train_second_half_seqs, train_all_seqs, a,b,c, train_first_half, train_second_half = get_all_sequences(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc014cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_first_half_seqs, valid_second_half_seqs, valid_all_seqs, a,b,c, valid_first_half, \\\n",
    "valid_second_half= get_all_sequences(valid)\n",
    "\n",
    "a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad356dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_first_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c832721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train_second_half.groupby(\"session_id\")[\"skip_2\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_targets = valid_second_half.groupby(\"session_id\")[\"skip_2\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_target(sequence, pad_to_this_length, pre): \n",
    "    if len(sequence) < pad_to_this_length:\n",
    "        sequence = [np.float32(i) for i in sequence]\n",
    "        if pre == True:\n",
    "            add = np.repeat(-1, pad_to_this_length - len(sequence)).tolist()\n",
    "            combo = [add, sequence]\n",
    "            combo = [item1 for item in combo for item1 in item]\n",
    "        elif pre== False:\n",
    "            add = np.repeat(-1, pad_to_this_length - len(sequence)).tolist()\n",
    "            combo = [sequence,add]\n",
    "            combo = [item1 for item in combo for item1 in item]\n",
    "            \n",
    "    else:\n",
    "        sequence = [np.float32(i) for i in sequence]\n",
    "        combo = sequence\n",
    "    return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf890d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = Parallel(n_jobs=6, verbose = 3)(delayed(pad_sequences_target)(i, 10, True) for i in train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_targets = Parallel(n_jobs=6, verbose = 3)(delayed(pad_sequences_target)(i, 10, False) for i in valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_targets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f9c95",
   "metadata": {},
   "source": [
    "## Read the acoustic features now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83435653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track features\n",
    "# read the track level acoustic features\n",
    "track_features_0 = pd.read_csv(r\"track_features/tf_000000000000.csv\")\n",
    "track_features_1 = pd.read_csv(r\"track_features/tf_000000000001.csv\")\n",
    "# combine into one unified dataframe\n",
    "track_features_all = pd.concat([track_features_0, track_features_1], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9dfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep only those features which are there in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_train = track_features_all[track_features_all[\"track_id\"].isin(train.track_id_clean)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d848725",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ab6c4",
   "metadata": {},
   "source": [
    "## Continue Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f82e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_padded_seq(data_train_first_half, data_train_second_half, data_valid_first_half, data_valid_second_half, \n",
    "                       max_len): \n",
    "    check_train_first_half = Parallel(n_jobs=6, verbose = 3)(delayed(pad_sequences)(i, max_len, True) for i in data_train_first_half[\"track_id_clean\"])\n",
    "    check_train_second_half = Parallel(n_jobs=6, verbose = 3)(delayed(pad_sequences)(i, max_len, False) for i in data_train_second_half[\"track_id_clean\"])\n",
    "    check_valid_first_half = Parallel(n_jobs=6, verbose = 3)(delayed(pad_sequences)(i, max_len, True) for i in data_valid_first_half[\"track_id_clean\"])\n",
    "    check_valid_second_half = Parallel(n_jobs=6, verbose = 3)(delayed(pad_sequences)(i, max_len, False) for i in data_valid_second_half[\"track_id_clean\"])\n",
    "    \n",
    "    \n",
    "    string_lookup_learned = tf.keras.layers.StringLookup(\n",
    "    max_tokens=train.track_id_clean.nunique()+2, num_oov_indices=1, \n",
    "    output_mode='int', mask_token = '0')\n",
    "    \n",
    "    string_lookup_learned.adapt(train.track_id_clean.values, batch_size = 1000000)\n",
    "    \n",
    "    text_to_seq_train_first_half = string_lookup_learned(check_train_first_half)\n",
    "    text_to_seq_train_second_half = string_lookup_learned(check_train_second_half)\n",
    "\n",
    "    text_to_seq_valid_first_half = string_lookup_learned(check_valid_first_half)\n",
    "    text_to_seq_valid_second_half = string_lookup_learned(check_valid_second_half)\n",
    "    \n",
    "    \n",
    "    string_lookup_static = tf.keras.layers.StringLookup(\n",
    "    max_tokens=track_features_all.track_id.nunique()+1, num_oov_indices=0, \n",
    "    output_mode='int', mask_token = '0')\n",
    "    \n",
    "    string_lookup_static.adapt(track_features_all.track_id, batch_size = 1000000)\n",
    "    \n",
    "    text_to_seq_train_first_half_static = string_lookup_static(check_train_first_half)\n",
    "    text_to_seq_train_second_half_static = string_lookup_static(check_train_second_half)\n",
    "\n",
    "    text_to_seq_valid_first_half_static = string_lookup_static(check_valid_first_half)\n",
    "    text_to_seq_valid_second_half_static = string_lookup_static(check_valid_second_half)\n",
    "    \n",
    "    return(text_to_seq_train_first_half, text_to_seq_train_second_half, \n",
    "          text_to_seq_valid_first_half, text_to_seq_valid_second_half, \n",
    "          text_to_seq_train_first_half_static, text_to_seq_train_second_half_static, \n",
    "          text_to_seq_valid_first_half_static, text_to_seq_valid_second_half_static, string_lookup_static, string_lookup_learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_seq_train_first_half, text_to_seq_train_second_half, text_to_seq_valid_first_half, text_to_seq_valid_second_half, \\\n",
    "text_to_seq_train_first_half_static, text_to_seq_train_second_half_static, \\\n",
    "text_to_seq_valid_first_half_static, text_to_seq_valid_second_half_static, string_lookup_static, string_lookup_learned = \\\n",
    "text_to_padded_seq(train_first_half_seqs, train_second_half_seqs, valid_first_half_seqs, valid_second_half_seqs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824722a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_seqs = tf.keras.layers.Concatenate()([text_to_seq_train_first_half, text_to_seq_train_second_half])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_all_seqs = tf.keras.layers.Concatenate()([text_to_seq_valid_first_half, text_to_seq_valid_second_half])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d369787",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_seqs_static = tf.keras.layers.Concatenate()([text_to_seq_train_first_half_static, text_to_seq_train_second_half_static])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02917aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_all_seqs_static = tf.keras.layers.Concatenate()([text_to_seq_valid_first_half_static, text_to_seq_valid_second_half_static])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files in testing\n",
    "files = os.listdir(\"test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the session logs\n",
    "# how many do you want to read\n",
    "empty = []\n",
    "for i in tqdm(range(0,1)):\n",
    "    sample_df_1 = pd.read_csv(\"test_set/\" + files[i])\n",
    "    empty.append(sample_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del the read sample df from the loop to save memory\n",
    "del(sample_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled data\n",
    "# concatenate into dataframe\n",
    "test = pd.concat(empty, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1471d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique sessions\n",
    "test.session_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.track_id_clean.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by first session id and then session position\n",
    "test = test.sort_values([\"session_id\", \"session_position\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the data\n",
    "test.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies for session length\n",
    "test.session_length.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2fb548",
   "metadata": {},
   "source": [
    "## Data Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbdd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## intersection of tracks between train, test, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## percentage common between train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train.track_id_clean.values).intersection(valid.track_id_clean.values))/valid.track_id_clean.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77313b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## percentage common between test and train\n",
    "\n",
    "len(set(train.track_id_clean.values).intersection(test.track_id_clean.values))/test.track_id_clean.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about sessions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52fc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train.session_id.values).intersection(valid.session_id.values))/valid.session_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train.session_id.values).intersection(test.session_id.values))/test.session_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c125193",
   "metadata": {},
   "source": [
    "## Start the data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c7e08",
   "metadata": {},
   "source": [
    "### creating embedding layer for static features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing the static track embeddings\n",
    "## has to be normalized and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3434febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We basically need three types of data\n",
    "## 1. Static song features - so need to one hot encode here\n",
    "## 2. Learnable embeddings - so need to know the population of training songs\n",
    "## 3. Session level features - these are the static user features\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's first tackle the track embedding network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58453a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for track embedding network, we need to know all the tracks in the\n",
    "## track features\n",
    "## and all the tracks in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_float_columns = track_features_all.columns[track_features_all.dtypes != \"float64\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063383cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in not_float_columns: \n",
    "    print(column)\n",
    "    print(track_features_all[column].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release year seems to have many values\n",
    "# let's treat as numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca257c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category columns\n",
    "# basically need to read spotify documentation\n",
    "cat_columns = [\"key\", \"mode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode these\n",
    "transformer = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the transformer\n",
    "transformer.fit(track_features_all[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c60b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for one hot encode\n",
    "ohe_features = transformer.transform(track_features_all[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9637b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns but the categorical treat as float\n",
    "float_columns = track_features_all.columns[~track_features_all.columns.isin(cat_columns)].drop(\"track_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30217fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to array\n",
    "track_features_all_array = np.array(track_features_all[float_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7920c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse one hot encoded stuff to array\n",
    "ohe_features = ohe_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb07fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the float features and one hot encoded stuff side by side\n",
    "track_features_all_array = np.hstack((track_features_all_array, ohe_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e208f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper mentions that they convery everything to zero mean\n",
    "# and unit variance so confirm that\n",
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the scaler\n",
    "std.fit(track_features_all_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "track_features_all_array = std.transform(track_features_all_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all means should be zero\n",
    "np.mean(track_features_all_array,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all std should be 1\n",
    "np.std(track_features_all_array,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_lookup_static.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all.track_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all.track_id.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d303a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all = pd.concat([track_features_all.track_id, pd.DataFrame(track_features_all_array)],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2627f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all_weights = track_features_all.set_index(\"track_id\").reindex(string_lookup_static.get_vocabulary()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a row of zeros for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all_weights = np.vstack((np.zeros((1,41)), track_features_all_weights.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features_all_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d9b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we can instantiate an embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36922585",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_embedding_layer = tf.keras.layers.Embedding(len(track_features_all_weights), track_features_all_weights.shape[1], \n",
    "                         weights = [track_features_all_weights], trainable = False, mask_zero = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8615e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we store it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_static_embeddings = tf.keras.layers.Input(shape = (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbfe4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_embeddings = static_embedding_layer(input_static_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_static_embeddings = tf.keras.models.Model(input_static_embeddings, static_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_static_embeddings.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99255b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_static_embeddings.save(\"static_embedding_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4dff0a",
   "metadata": {},
   "source": [
    "### creating the playback and meta features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_playback_features_train(data):\n",
    "    data[\"day_of_week\"] = pd.to_datetime(data[\"date\"]).dt.day_name()\n",
    "\n",
    "    meta_columns = [\"premium\", \"day_of_week\"]\n",
    "\n",
    "    # one hot encode these\n",
    "    transformer_meta = OneHotEncoder()\n",
    "\n",
    "    # fit the transformer\n",
    "    transformer_meta.fit(data[meta_columns])\n",
    "\n",
    "    # transform for one hot encode\n",
    "    ohe_features_meta = transformer_meta.transform(data[meta_columns])\n",
    "\n",
    "#     data[\"session_length\"].values.reshape(-1,1).shape\n",
    "\n",
    "    meta_features_train = np.hstack((ohe_features_meta.toarray(), data[\"session_length\"].values.reshape(-1,1)))\n",
    "\n",
    "    std_meta_features_train = StandardScaler()\n",
    "\n",
    "    meta_features_train = std_meta_features_train.fit_transform(meta_features_train)\n",
    "\n",
    "    # other playback features\n",
    "\n",
    "    playback_features = data.columns[~data.columns.isin(meta_columns)]\n",
    "\n",
    "# playback_features.dtypes\n",
    "\n",
    "# for col in playback_data.columns:\n",
    "#     print(col)\n",
    "#     print(playback_data[col].nunique())\n",
    "\n",
    "    categorical_features = [features for features in playback_features.tolist() if features not in ['session_id', \"session_position\", \n",
    "                                                                                      'track_id_clean', \n",
    "                                                                                       'hist_user_behavior_n_seekfwd',\n",
    "                                                                                       'hist_user_behavior_n_seekback', \n",
    "                                                                                      'hour_of_day','date'\n",
    "                                                                                      ]]\n",
    "\n",
    "    playback_data_cat = data[categorical_features]\n",
    "\n",
    "#     playback_data_cat.head()\n",
    "\n",
    "    # one hot encode these\n",
    "    transformer_playback_cat = OneHotEncoder()\n",
    "\n",
    "    # fit the transformer\n",
    "    transformer_playback_cat.fit(data[categorical_features])\n",
    "\n",
    "    # transform for one hot encode\n",
    "    playback_ohe_features_train = transformer_playback_cat.transform(data[categorical_features])\n",
    "\n",
    "    # playback numerical features\n",
    "\n",
    "    playback_numerical_train = data[[\"session_position\", 'hist_user_behavior_n_seekfwd','hist_user_behavior_n_seekback', 'hour_of_day']]\n",
    "\n",
    "    std_playback_numerical_train = StandardScaler()\n",
    "\n",
    "    std_playback_numerical_train.fit(playback_numerical_train)\n",
    "\n",
    "    playback_numerical_train = std_playback_numerical_train.transform(playback_numerical_train)\n",
    "\n",
    "    playback_features_train = np.hstack((playback_ohe_features_train.toarray(), playback_numerical_train))\n",
    "    \n",
    "    return meta_features_train, playback_features_train, transformer_meta, std_meta_features_train, \\\n",
    "           transformer_playback_cat, std_playback_numerical_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_train, playback_features_train, transformer_meta, std_meta_features_train, \\\n",
    "           transformer_playback_cat, std_playback_numerical_train = meta_playback_features_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce173c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_train = pd.concat([train[[\"session_id\", \"session_position\"]], pd.DataFrame(meta_features_train)],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_train = meta_features_train.drop_duplicates([\"session_id\"]).reset_index(drop = True).drop(\"session_position\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train = pd.concat([train[[\"session_id\", \"session_position\"]], pd.DataFrame(playback_features_train)],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0410ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train[\"row_number\"] = playback_features_train.groupby(\"session_id\").cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801da575",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train[\"session_row_number\"] = playback_features_train[\"session_id\"] + \\\n",
    "                                                playback_features_train[\"row_number\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4db61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_first_half.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58059b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is an important step, keeping only those playbacks which are in first half of train and valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bf700",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train = playback_features_train[playback_features_train.session_row_number.isin(train_first_half.session_row_number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4696bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62987301",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda02ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62995662",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_playback_tracks = pd.DataFrame(playback_features_train.groupby([\"session_id\"])[\"session_row_number\"].apply(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb79b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_train_first_half = Parallel(n_jobs=6, verbose = 3)(delayed(pad_sequences)(i,\n",
    "                         10, True) for i in list_of_playback_tracks[\"session_row_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_train_first_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e12208",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_lookup_playback = tf.keras.layers.StringLookup(\n",
    "max_tokens=playback_features_train.session_row_number.nunique()+1, num_oov_indices=0, \n",
    "output_mode='int', mask_token = '0')\n",
    "    \n",
    "string_lookup_playback.adapt(playback_features_train.session_row_number, batch_size = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51dbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train = playback_features_train.drop([\"session_id\",\"session_position\", \"row_number\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train = playback_features_train.set_index(\"session_row_number\").reindex(string_lookup_playback.get_vocabulary()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe490961",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_weights = np.vstack((np.zeros((1,playback_features_train.shape[1])), \n",
    "                              playback_features_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0345fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_embedding = tf.keras.layers.Embedding(playback_weights.shape[0], playback_weights.shape[1], mask_zero = True, \n",
    "                         weights = [playback_weights], trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(meta_features_train.session_id == train_first_half_seqs.session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_playback_first_half_seq = string_lookup_playback(check_train_first_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    playback_embedding_first_half = playback_embedding(train_playback_first_half_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_playback_features(data, transformer_meta, std_meta_features_train, transformer_playback_cat, \n",
    "                          std_playback_numerical_train):\n",
    "    data[\"day_of_week\"] = pd.to_datetime(data[\"date\"]).dt.day_name()\n",
    "\n",
    "    meta_columns = [\"premium\", \"day_of_week\"]\n",
    "\n",
    "    # one hot encode these\n",
    "#     transformer_meta = OneHotEncoder()\n",
    "\n",
    "    # fit the transformer\n",
    "    transformer_meta.fit(data[meta_columns])\n",
    "\n",
    "    # transform for one hot encode\n",
    "    ohe_features_meta = transformer_meta.transform(data[meta_columns])\n",
    "\n",
    "#     data[\"session_length\"].values.reshape(-1,1).shape\n",
    "\n",
    "    meta_features_train = np.hstack((ohe_features_meta.toarray(), data[\"session_length\"].values.reshape(-1,1)))\n",
    "\n",
    "#     std_meta_features_train = StandardScaler()\n",
    "\n",
    "    meta_features_train = std_meta_features_train.fit_transform(meta_features_train)\n",
    "\n",
    "    # other playback features\n",
    "\n",
    "    playback_features = data.columns[~data.columns.isin(meta_columns)]\n",
    "\n",
    "# playback_features.dtypes\n",
    "\n",
    "# for col in playback_data.columns:\n",
    "#     print(col)\n",
    "#     print(playback_data[col].nunique())\n",
    "\n",
    "    categorical_features = [features for features in playback_features.tolist() if features not in ['session_id', \"session_position\", \n",
    "                                                                                      'track_id_clean', \n",
    "                                                                                       'hist_user_behavior_n_seekfwd',\n",
    "                                                                                       'hist_user_behavior_n_seekback', \n",
    "                                                                                      'hour_of_day','date'\n",
    "                                                                                      ]]\n",
    "\n",
    "    playback_data_cat = data[categorical_features]\n",
    "\n",
    "#     playback_data_cat.head()\n",
    "\n",
    "    # one hot encode these\n",
    "#     transformer_playback_cat = OneHotEncoder()\n",
    "\n",
    "    # fit the transformer\n",
    "    transformer_playback_cat.fit(data[categorical_features])\n",
    "\n",
    "    # transform for one hot encode\n",
    "    playback_ohe_features_train = transformer_playback_cat.transform(data[categorical_features])\n",
    "\n",
    "    # playback numerical features\n",
    "\n",
    "    playback_numerical_train = data[[\"session_position\", 'hist_user_behavior_n_seekfwd','hist_user_behavior_n_seekback', 'hour_of_day']]\n",
    "\n",
    "#     std_playback_numerical_train = StandardScaler()\n",
    "\n",
    "    std_playback_numerical_train.fit(playback_numerical_train)\n",
    "\n",
    "    playback_numerical_train = std_playback_numerical_train.transform(playback_numerical_train)\n",
    "\n",
    "    playback_features_train = np.hstack((playback_ohe_features_train.toarray(), playback_numerical_train))\n",
    "    \n",
    "    return meta_features_train, playback_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b0340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = meta_playback_features(valid, transformer_meta, std_meta_features_train, transformer_playback_cat, \n",
    "                          std_playback_numerical_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_valid, playback_features_valid = valid_features[0], valid_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd87f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_valid = pd.concat([valid[[\"session_id\", \"session_position\"]], pd.DataFrame(meta_features_valid)],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_valid = meta_features_valid.drop_duplicates([\"session_id\"]).reset_index(drop = True).drop(\"session_position\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029528a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(meta_features_valid.session_id == valid_first_half_seqs.session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid = pd.concat([valid[[\"session_id\", \"session_position\"]], pd.DataFrame(playback_features_valid)],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ac97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid[\"row_number\"] = playback_features_valid.groupby(\"session_id\").cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a019574",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid[\"session_row_number\"] = playback_features_valid[\"session_id\"] + \\\n",
    "                                                playback_features_valid[\"row_number\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4089ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid = playback_features_valid[playback_features_valid.session_row_number.isin(valid_first_half.session_row_number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b39e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f691694",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_first_half.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_first_half_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_playback_tracks_valid = pd.DataFrame(playback_features_valid.groupby([\"session_id\"])[\"session_row_number\"].apply(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(l[0]) for l in list_of_playback_tracks_valid[[\"session_row_number\"]].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace72fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_valid_first_half = Parallel(n_jobs=6, verbose = 3)(delayed(pad_sequences)(i,\n",
    "                         10, True) for i in list_of_playback_tracks_valid[\"session_row_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65516b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_lookup_playback_valid = tf.keras.layers.StringLookup(\n",
    "max_tokens=playback_features_valid.session_row_number.nunique()+1, num_oov_indices=0, \n",
    "output_mode='int', mask_token = '0')\n",
    "    \n",
    "string_lookup_playback_valid.adapt(playback_features_valid.session_row_number, batch_size = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131992eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid = playback_features_valid.drop([\"session_id\",\"session_position\", \"row_number\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fc3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid = playback_features_valid.set_index(\"session_row_number\").reindex(string_lookup_playback_valid.get_vocabulary()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_features_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e496dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_weights_valid = np.vstack((np.zeros((1,playback_features_valid.shape[1])), \n",
    "                              playback_features_valid.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c99975",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_embedding_valid = tf.keras.layers.Embedding(playback_weights_valid.shape[0], playback_weights_valid.shape[1], mask_zero = True, \n",
    "                         weights = [playback_weights_valid], trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(check_valid_first_half).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e56608",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_playback_first_half_seq = string_lookup_playback_valid(check_valid_first_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bba5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_playback_first_half_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e28039",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    playback_embedding_first_half_valid = playback_embedding_valid(valid_playback_first_half_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc96f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_embedding_first_half_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3526ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_embedding_first_half.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce022af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep only those playbacks which exist in first half of train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to pad the playback train and valid features as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cb5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## and check if the ordering by session_ids is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efdad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_seq_train_first_half, text_to_seq_train_second_half, text_to_seq_valid_first_half, text_to_seq_valid_second_half, \\\n",
    "text_to_seq_train_first_half_static, text_to_seq_train_second_half_static, \\\n",
    "text_to_seq_valid_first_half_static, text_to_seq_valid_second_half_static, string_lookup_static, \n",
    "train_all_seqs, valid_all_seqs, train_all_seqs_static, valid_all_seqs_static, train_targets, valid_targets, meta_features_train, \n",
    "meta_features_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_train.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee512dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_array = np.array(train_targets).reshape(np.array(train_targets).shape[0], np.array(train_targets).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_targets_array = np.array(valid_targets).reshape(np.array(valid_targets).shape[0], np.array(valid_targets).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce74b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_targets_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aa277",
   "metadata": {},
   "source": [
    "## Make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(tf.keras.layers.Layer):\n",
    "    def call(self, input_tensor, mask ):\n",
    "#         broadcast_float_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
    "        x = tf.keras.layers.Dense(32)(input_tensor)\n",
    "        x = tf.keras.layers.Dense(1)(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        a =  tf.keras.layers.Softmax()(x, mask = mask)\n",
    "        x = tf.keras.layers.RepeatVector(32)(a)\n",
    "        x = tf.keras.layers.Permute((2,1))(x)\n",
    "        x = tf.keras.layers.Multiply()((input_tensor, x))\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis = 1))(x)\n",
    "        return a, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first handle the first half sequence for the learnable embeddings\n",
    "learnable_first_half_sequence = tf.keras.layers.Input(shape = (10,), name = \"first_half_learnable_tracks\")\n",
    "\n",
    "# now handle the first half sequence for the static embeddings\n",
    "static_first_half_sequence = tf.keras.layers.Input(shape = (10,), name = \"first_half_static_tracks\")\n",
    "\n",
    "# pass these two through their respective embedding layers\n",
    "\n",
    "learnable_embs_layer = tf.keras.layers.Embedding(len(string_lookup_learned.get_vocabulary()), 50, mask_zero = True, \n",
    "                                                 name = \"learnable_embeddings\")\n",
    "\n",
    "# emb processed output for learnable embs\n",
    "learnable_first_half_embs_output = learnable_embs_layer(learnable_first_half_sequence)\n",
    "\n",
    "# keep this mask\n",
    "first_half_mask = tf.keras.layers.Masking(name = \"mask_for_first_half\")(learnable_first_half_sequence)\n",
    "\n",
    "# emb processed output for static embs\n",
    "static_first_half_embs_output = static_embedding_layer(static_first_half_sequence)\n",
    "\n",
    "# combine these two together\n",
    "concatenated_embeddings_first_half = tf.keras.layers.Concatenate(name = \\\n",
    "                                                                 \"concatenate_learnable_with_static_first_half\") \\\n",
    "                                                                    ([learnable_first_half_embs_output,\n",
    "                                                                    static_first_half_embs_output])\n",
    "\n",
    "# now we need to pass these concatenated embeddings through a Dense layer\n",
    "combining_learnable_static_embs_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(50), \n",
    "                                                                       name = \"time_distributed_for_embs_transforms\")\n",
    "\n",
    "# transformed embs first half\n",
    "transformed_embeddings_first_half = combining_learnable_static_embs_layer(concatenated_embeddings_first_half, \n",
    "                                                                          mask = tf.cast(first_half_mask, tf.bool))\n",
    "\n",
    "# get the input for the second half\n",
    "\n",
    "# first handle the second half sequence for the learnable embeddings\n",
    "learnable_second_half_sequence = tf.keras.layers.Input(shape = (10,), name = \"input_second_half_learnable_tracks\")\n",
    "\n",
    "# keep this mask\n",
    "second_half_mask = tf.keras.layers.Masking(name = \"second_half_mask\")(learnable_second_half_sequence)\n",
    "\n",
    "# now handle the first half sequence for the static embeddings\n",
    "static_second_half_sequence = tf.keras.layers.Input(shape = (10,), name = \"input_second_half_static_tracks\")\n",
    "\n",
    "# get the second half learnable embs output\n",
    "learnable_second_half_embs_output = learnable_embs_layer(learnable_second_half_sequence)\n",
    "\n",
    "# get the second half static embs output \n",
    "static_second_half_embs_output = static_embedding_layer(static_second_half_sequence)\n",
    "\n",
    "# combine these two together\n",
    "concatenated_embeddings_second_half = tf.keras.layers.Concatenate(name = \"concatenate_learnable_with_static_second_half\")([learnable_second_half_embs_output,\n",
    "                                                                    static_second_half_embs_output])\n",
    "\n",
    "# transformed embs second half\n",
    "transformed_embeddings_second_half = combining_learnable_static_embs_layer(concatenated_embeddings_second_half, \n",
    "                                                                          mask = tf.cast(second_half_mask, tf.bool))\n",
    "\n",
    "# transformed embeddings session\n",
    "transformed_embeddings_session = tf.keras.layers.Concatenate(axis = -2, \n",
    "                                                            name = \"concatenate_embs_for_session\")([transformed_embeddings_first_half, \n",
    "                                                               transformed_embeddings_second_half])\n",
    "\n",
    "# combined mask session\n",
    "combined_mask_session = tf.keras.layers.Concatenate(name = \"concatenate_first_second_half_mask\")([first_half_mask, second_half_mask])\n",
    "\n",
    "# pass the session embeddings through an LSTM first\n",
    "lstm_for_session_encoder_layer = tf.keras.layers.LSTM(32, return_sequences = True, name = \"LSTM_for_first_half\", \n",
    "                                                      recurrent_dropout = 0.750)\n",
    "\n",
    "# lstm processed session encodings at each timestep\n",
    "lstm_processed_embs_for_session = lstm_for_session_encoder_layer(transformed_embeddings_session, mask = tf.cast(combined_mask_session, tf.bool))\n",
    "\n",
    "\n",
    "## putting the attention here\n",
    "x = tf.keras.layers.Dense(lstm_processed_embs_for_session.get_shape()[-1])(lstm_processed_embs_for_session)\n",
    "x = tf.keras.layers.Dense(1)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "a =  tf.keras.layers.Softmax()(x, mask = combined_mask_session)\n",
    "x = tf.keras.layers.RepeatVector(lstm_processed_embs_for_session.get_shape()[-1])(a)\n",
    "x = tf.keras.layers.Permute((2,1))(x)\n",
    "x = tf.keras.layers.Multiply()((lstm_processed_embs_for_session, x))\n",
    "x = tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis = 1))(x)\n",
    "\n",
    "# put the attention on top of the session encodings\n",
    "# _, session_embedding = SimpleAttention(name = \"Attention_for_session_encodings\")(lstm_processed_embs_for_session, combined_mask_session)\n",
    "\n",
    "## get the hidden states ready for the lstm to consume\n",
    "\n",
    "# first concatenate the session embedding and the meta features\n",
    "meta_features = tf.keras.layers.Input(shape = (10), name = \"input_for_meta_features\")\n",
    "\n",
    "# concatenate with the session encodinh\n",
    "meta_with_session_encoding = tf.keras.layers.Concatenate(name = \"concatenate_meta_with_session\")([meta_features, x])\n",
    "\n",
    "# pass this twice through some dense layers\n",
    "# # Pass through one Dense for state\n",
    "dense_for_state = Dense(32, name = \"transform_for_lstm_hidden_state\",activation = \"relu\")\n",
    "\n",
    "# # Pass through one Dense for carry\n",
    "dense_for_carry = Dense(32, name = \"transform_for_lstm_carry_state\",activation = \"relu\")\n",
    "\n",
    "## hidden state\n",
    "first_half_lstm_hidden_state = dense_for_state(meta_with_session_encoding)\n",
    "\n",
    "## carry state\n",
    "first_half_lstm_carry = dense_for_carry(meta_with_session_encoding)\n",
    "\n",
    "# combine the states\n",
    "encoder_states = [first_half_lstm_hidden_state, first_half_lstm_carry]\n",
    "\n",
    "# now get the input for the playback tracks or sequences\n",
    "playback_first_half_input = tf.keras.layers.Input(shape = (10,60,), name = \"playback_input\")\n",
    "\n",
    "# concatenate this with the embeddings from the first half\n",
    "encoder_seq_input = tf.keras.layers.Concatenate(name = \"concatenate_playback_with_first_half_encodings\")([tf.cast(transformed_embeddings_first_half, tf.float32),\n",
    "                                                   tf.cast(playback_first_half_input, tf.float32)])\n",
    "\n",
    "## pass this through an lstm for the encoder\n",
    "lstm_for_first_half = tf.keras.layers.LSTM(32, return_state = True, name = \"lstm_for_first_half\", \n",
    "                                           recurrent_dropout = 0.75)\n",
    "\n",
    "\n",
    "# lstm processed first half encoder\n",
    "_, lstm_first_half_hidden, lstm_first_half_carry = lstm_for_first_half(encoder_seq_input, mask = tf.cast(first_half_mask, tf.bool), \n",
    "                                                        initial_state = encoder_states)\n",
    "\n",
    "# now get the state to be passed to the decoder\n",
    "decoder_states = [lstm_first_half_hidden, lstm_first_half_carry]\n",
    "\n",
    "# pass this decoder state to the lstm for the second half along with mask \n",
    "lstm_for_second_half = tf.keras.layers.LSTM(32, return_sequences = True, name = \"lstm_for_second_half\", \n",
    "                                           recurrent_dropout = 0.75)\n",
    "\n",
    "# pass the second half sequence through this\n",
    "features_to_predict = lstm_for_second_half(transformed_embeddings_second_half, mask = tf.cast(second_half_mask, tf.bool), \n",
    "                    initial_state = decoder_states)\n",
    "\n",
    "# now pass them through a time distributed dense layer\n",
    "time_distibuted_dense_for_classification = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation = \"sigmoid\"), name = \"time_distributed_for_classification\")\n",
    "\n",
    "# get the predictions\n",
    "final_preds_for_logits = time_distibuted_dense_for_classification(features_to_predict, mask = tf.cast(second_half_mask, tf.bool))\n",
    "\n",
    "# flatten\n",
    "final_preds_for_logits = tf.keras.layers.Flatten()(final_preds_for_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8147243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model([learnable_first_half_sequence, static_first_half_sequence, \n",
    "                              learnable_second_half_sequence, static_second_half_sequence, \n",
    "                              meta_features, playback_first_half_input], final_preds_for_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f40761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_binary_cross_entropy(y_true, y_pred):\n",
    "    mask_current = tf.cast(y_true != -1,tf.float32) \n",
    "    y_true = tf.math.multiply(mask_current, y_true)\n",
    "#         y_pred = mask_current*(y_pred)\n",
    "    denom = tf.math.reduce_sum(mask_current, axis = 1)\n",
    "    bce = -(y_true*tf.cast(tf.math.log(y_pred),tf.float32) + (1-y_true)*tf.cast(tf.math.log(1-y_pred), tf.float32))\n",
    "    bce = tf.math.multiply(bce, mask_current)\n",
    "#     weights = tf.range(1,(tf.shape(bce)[1]*3)+1,3)[:tf.shape(bce)[1]][::-1]\n",
    "    weights_overall = tf.convert_to_tensor(np.array([20,1,1,1,1,1,1,1,1,1]))\n",
    "    weights = weights_overall[:tf.shape(bce)[1]]\n",
    "    weights_sum = tf.math.reduce_sum(weights)\n",
    "    bce = bce*tf.cast(weights, tf.float32)\n",
    "    num = tf.math.reduce_sum(bce, axis = 1)\n",
    "    return num/tf.cast(weights_sum, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14502f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_accuracy(actual, preds):\n",
    "    preds = tf.cast(preds > 0.5, tf.float32)\n",
    "    first_part = tf.cast(tf.math.equal(actual, preds), tf.float32)\n",
    "    second_part = tf.math.cumsum(first_part, axis = 1)\n",
    "    third_part = tf.math.multiply(first_part, second_part)\n",
    "    fourth_part = tf.math.cumsum(tf.ones((tf.shape(actual)[0],tf.shape(actual)[1])), axis = 1)\n",
    "    fifth_part = tf.math.divide(third_part, fourth_part)\n",
    "    sixth = K.mean(fifth_part, axis = 1)\n",
    "    return sixth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class average_accuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"average_accuracy\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.average_accuracy = self.add_weight(name=\"aa\", initializer=\"zeros\")\n",
    "        self.total_samples = self.add_weight(name=\"ts\", initializer=\"zeros\", dtype = \"int32\")\n",
    "        \n",
    "    def update_state(self, actual, preds, sample_weight=None):\n",
    "        actual = tf.cast(actual , tf.float32)\n",
    "        mask_inner = tf.not_equal(actual, -1)\n",
    "\n",
    "        cumsum_mask_inner = tf.math.cumsum(tf.cast(mask_inner, tf.float32), axis = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        preds = tf.cast(preds > 0.5, tf.float32)\n",
    "        first_part = tf.cast(tf.math.equal(actual, preds), tf.float32)\n",
    "        second_part = tf.math.cumsum(first_part, axis = 1)\n",
    "        third_part = tf.math.multiply(first_part, second_part)\n",
    "        mymean = tf.math.divide(third_part, cumsum_mask_inner)\n",
    "        mymean = tf.where(tf.math.is_nan(mymean), tf.zeros_like(mymean), mymean)\n",
    "        mymean = tf.reduce_sum(tf.where(tf.math.is_nan(mymean), tf.zeros_like(mymean), mymean),1)\n",
    "        mymean1 = tf.reduce_sum(tf.cast(mask_inner, tf.float32), 1)\n",
    "#         fourth_part = tf.math.cumsum(tf.ones((tf.shape(actual)[0],tf.shape(actual)[1])), axis = 1)\n",
    "        fifth_part = tf.math.divide(mymean, mymean1)\n",
    "#         sixth = K.mean(fifth_part, axis = 1)\n",
    "        num_samples = tf.shape(preds)[0]\n",
    "        self.average_accuracy.assign_add(tf.reduce_sum(fifth_part))\n",
    "        self.total_samples.assign_add(num_samples)\n",
    "        \n",
    "    def result(self):\n",
    "        return self.average_accuracy/tf.cast(self.total_samples, tf.float32)\n",
    "\n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.average_accuracy.assign(0.0)\n",
    "        self.total_samples.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class average_accuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"average_accuracy\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.average_accuracy = self.add_weight(name=\"aa\", initializer=\"zeros\")\n",
    "        self.total_samples = self.add_weight(name=\"ts\", initializer=\"zeros\", dtype = \"int32\")\n",
    "        \n",
    "    def update_state(self, actual, preds, sample_weight=None):\n",
    "        actual = tf.cast(actual , tf.float32)\n",
    "        mask_inner = tf.not_equal(actual, -1)\n",
    "\n",
    "        cumsum_mask_inner = tf.math.cumsum(tf.cast(mask_inner, tf.float32), axis = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        preds = tf.cast(preds > 0.5, tf.float32)\n",
    "        first_part = tf.cast(tf.math.equal(actual, preds), tf.float32)\n",
    "        second_part = tf.math.cumsum(first_part, axis = 1)\n",
    "        third_part = tf.math.multiply(first_part, second_part)\n",
    "        mymean = tf.math.divide(third_part, cumsum_mask_inner)\n",
    "        mymean = tf.where(tf.math.is_nan(mymean), tf.zeros_like(mymean), mymean)\n",
    "        mymean = tf.reduce_sum(tf.where(tf.math.is_nan(mymean), tf.zeros_like(mymean), mymean),1)\n",
    "        mymean1 = tf.reduce_sum(tf.cast(mask_inner, tf.float32), 1)\n",
    "#         fourth_part = tf.math.cumsum(tf.ones((tf.shape(actual)[0],tf.shape(actual)[1])), axis = 1)\n",
    "        fifth_part = tf.math.divide(mymean, mymean1)\n",
    "#         sixth = K.mean(fifth_part, axis = 1)\n",
    "        num_samples = tf.shape(preds)[0]\n",
    "        self.average_accuracy.assign_add(tf.reduce_sum(fifth_part))\n",
    "        self.total_samples.assign_add(num_samples)\n",
    "        \n",
    "    def result(self):\n",
    "        return self.average_accuracy/tf.cast(self.total_samples, tf.float32)\n",
    "\n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.average_accuracy.assign(0.0)\n",
    "        self.total_samples.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0005),\n",
    "    loss=custom_binary_cross_entropy, metrics = [average_accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([text_to_seq_train_first_half, text_to_seq_train_first_half_static,\n",
    "           text_to_seq_train_second_half, text_to_seq_train_second_half_static, \n",
    "           np.array(meta_features_train.iloc[:,1:]), playback_embedding_first_half\n",
    "          ], train_targets_array, verbose = 1, epochs = 10, \n",
    "         validation_data = ([text_to_seq_valid_first_half, text_to_seq_valid_first_half_static,\n",
    "           text_to_seq_valid_second_half, text_to_seq_valid_second_half_static, \n",
    "           np.array(meta_features_valid.iloc[:,1:]), playback_embedding_first_half_valid\n",
    "          ],valid_targets_array) , \n",
    "         callbacks = tf.keras.callbacks.EarlyStopping(monitor = \"val_average_accuracy\",\n",
    "                                                      patience = 3, restore_best_weights=True, \n",
    "                                                     mode = \"max\"), \n",
    "         batch_size = 2048, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39fedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([text_to_seq_valid_first_half[:5,:], text_to_seq_valid_first_half_static[:5,:],\n",
    "           text_to_seq_valid_second_half[:5,:], text_to_seq_valid_second_half_static[:5,:], \n",
    "           np.array(meta_features_valid.iloc[:,1:])[:5,:], playback_embedding_first_half_valid[:5,:]\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_targets_array[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes = True, to_file = \"functional_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727db24d",
   "metadata": {},
   "source": [
    "## Miscallaneous verify how to mask the binary cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape = (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = tf.keras.layers.Embedding(10,8, mask_zero = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_processed = emb_layer(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb907e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output = dense_layer(emb_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba039ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb896806",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(input_layer, dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2288c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = np.array([0,0,2,3,4]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array([-1, -1, 1, 0, 1]).reshape(1,-1).reshape(1,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99752bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(input_seq, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee26e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = tf.keras.layers.Input(shape = ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4447fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_new_processed = emb_layer(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output_new = dense_layer(emb_new_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.Model(new_input, dense_output_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709281ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_model.evaluate([2],[1]) + new_model.evaluate([3],[0]) + new_model.evaluate([4],[1]))/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce149d1f",
   "metadata": {},
   "source": [
    "## with more than 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape = (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = tf.keras.layers.Embedding(10,8, mask_zero = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_processed = emb_layer(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c75c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326105ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output = dense_layer(emb_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba54277",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(input_layer, dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5877d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf27a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[0,0,2,3,4], [0,1,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = np.array([[0,0,2,3,4], [0,1,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array([[-1, -1, 1, 0, 1], \n",
    "                 [-1,1,0,0,1]]).reshape(2,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc898bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(input_seq, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = tf.keras.layers.Input(shape = ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_new_processed = emb_layer(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output_new = dense_layer(emb_new_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdae46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.Model(new_input, dense_output_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74fe38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_model.evaluate([2],[1]) + new_model.evaluate([3],[0]) + new_model.evaluate([4],[1]))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fb4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_model.evaluate([1],[1]) + new_model.evaluate([2],[0]) + new_model.evaluate([3],[0]) + new_model.evaluate([4],[1]))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.5487704753875733 + 0.4106751441955566)/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:onetwo] *",
   "language": "python",
   "name": "conda-env-onetwo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
